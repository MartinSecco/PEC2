# -*- coding: utf-8 -*-
"""PEC2_Martín_Secco.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wLm3IrR2ELt4qswYAgc-XL29EBjEs4nY
"""

import pandas as pd
import numpy as np
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as sp
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, make_scorer
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV, cross_validate, RepeatedKFold
from sklearn.preprocessing import RobustScaler, OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.compose import ColumnTransformer, TransformedTargetRegressor, make_column_transformer
from sklearn import preprocessing
from sklearn.tree import DecisionTreeClassifier

"""**Crear el pipeline de ML (con transformers y estimators) para dar un resultado**"""

#Leemos el dataset

df = pd.read_csv('/content/insurance.csv')
df.head()

#Vemos como funciona la variable charges

sns.pairplot(df[["charges"]])

#Lea plicamos logaritmos en base 10 para transformar la variable y que el ML lo entienda mejor

sns.pairplot(np.log10(df[['charges']]))

#Vemos la información general del dataset, con la cantidad de valore nulos y el type de cada columna

df.info()

df.isna().sum()

#Separamos las variables en x e y. La "x" son las variables que nos ayudan a predecir el valor y la "y" es la variable dependiente

x, y = df.drop(["charges"], axis = 1), df["charges"]

#Separamos en train y test para entrenar, testear y poder predecir el modelo luego.

X_train, X_test, y_train, y_test = train_test_split(x, y, random_state = 42)

#Realizamos un preprocesador, donde se le indican columans con infomración y te realiza cierta acción.
#A las variable categóricas le realizamos un OneHotEncoder donde se transforma una variable de texto en ceros y unos para ayudar al sistema de ML.

categorical_columns = ["sex", "smoker", "region"]
numerical_columns = ["age", "bmi", "children"]

preprocessor = make_column_transformer((OneHotEncoder(drop = "if_binary"), categorical_columns),
                                       remainder = "passthrough",
                                       verbose_feature_names_out = False,)

#Creamos un pipeline que usa el preprocesador previo y el TransformesTarget Regressor con regressor "Ridge" para poder predecir el valor

model = make_pipeline(preprocessor,
                      TransformedTargetRegressor(regressor = Ridge (alpha = 1e-10), func = np.log10, inverse_func = sp.special.exp10),)

#Entrenamos el modelo con el comando "fit" para que entienda como se calcula el valor utilizando los parámetros de entrenamiento

model.fit(X_train, y_train)

#Vamos a realizar ciertas predicciones en la variable "y_pred" sobre los datos de entrenamiento y prueba.
#Vamos a aplicar el MAE para ver que tan bien está siendo el modelo y se crea una cadena de texto para mostrar dicho valor MAE.

y_pred = model.predict(X_train)

mae = median_absolute_error(y_train, y_pred)
string_score = f"MAE on training set: {mae: 2f} euros"

y_pred = model.predict(X_test)

mae = median_absolute_error(y_test, y_pred)
string_score += f"\nMAE on testing set: {mae: 2f} euros"

fig, ax = plt.subplots(figsize = (7, 7))
plt.scatter(y_test, y_pred)
ax.plot([0, 1], [0, 1], transform = ax.transAxes, ls = "--", c = "yellow")
plt.text(50000, 20000, string_score)
plt.title("Ridge model, small regularization")
plt.ylabel("Model predictions")
plt.xlabel("Truths")
plt.show()

"""Analizando los resultados de la predicción del modelo, podemos observar ciertos valores en los que difiere según el conjunto de datos.
Para el caso del conjunto de datos de entrenamiento, las predicciones del modelo difieren en un monto de 933.902042 euros en promedio con respecto a los valores reales.
Mientras que para el conjunto de datos de prueba, las predicciones difieren en promedio 953.419613 euros con respecto a los valores reales.
"""

#Analizamos los coeficientes y vemos cuanto aporta cada variable en la predicción del valor del seguro.

feature_names = model[:-1]. get_feature_names_out()

coefs = pd.DataFrame(
    model[-1].regressor_.coef_,
    columns = ["Coefficients"],
    index = feature_names)

coefs

"""Como principial coeficiente que tiene un alto impacto en el precio del seguro es en la variable de si la persona fuma o no."""

#pasamos a ver los coeficientes en formato de barras horizontales, para poder visualizar la importancia relativa de cada variable de una forma mas visual

coefs.plot.barh(figsize=(9, 7))
plt.title("Ridge model, small regularization")
plt.axvline(x = 0, color = ".5")
plt.xlabel("Raw coefficient values")
plt.subplots_adjust(left = 0.3)

#Creamos otro gráfico de barras horizontales, donde identificamos la varianza en cada una de las variables en el conjunto de entrenamiento preprocesado
#Cuanto mayor la barra, mayor la varianza de la variable

X_train_preprocessed = pd.DataFrame(model[:1].transform(X_train), columns = feature_names)

X_train_preprocessed.std(axis = 0).plot.barh(figsize = (9, 7))
plt.title("Feature ranges")
plt.xlabel("Std. dev. of feature values")
plt.subplots_adjust(left = 0.3)

#procedemos a corregir las varianzas de las variables para poder ver mucho mejor la importancia de cada variable.

coefs = pd.DataFrame(model[-1].regressor_.coef_ * X_train_preprocessed.std(axis = 0), columns = ["Coefficient importance"], index = feature_names)
coefs.plot(kind = "barh", figsize = (9, 7 ))
plt.xlabel("Coefficient values corrected by the features std. dev.")
plt.title("Ridge model, small regularization")
plt.axvline(x = 0, color = ".5")
plt.subplots_adjust(left = 0.3)

#Utilizamos a continuación la técnica de Cross Validation, ya que queremos repetir lo hecho anteriormente para varias separaciones de nuestro datos para lograr mejores resultados en nuestro análisis

cv = RepeatedKFold(n_splits = 5, n_repeats = 5, random_state = 0)
cv_model = cross_validate(
    model,
    x,
    y,
    cv = cv,
    return_estimator = True,
    n_jobs = 2)

coefs = pd.DataFrame(
    [
        est[-1].regressor_.coef_
        for est, (train_idx, _) in zip(cv_model["estimator"], cv.split(x, y))
    ],
    columns = feature_names)

coefs

#Creamos un nuevo gráfico, donde vamos a ver la importancia relativa de los coeficientes del modelo con el cross validation.

plt.figure(figsize = (7,7))
sns.stripplot(data = coefs, orient = "h", color = "k", alpha = 0.5)
sns.boxplot(data = coefs, orient = "h", color = "cyan", saturation = 0.5)
plt.axvline(x = 0, color = "0.5")
plt.title("Coeficiente de importancia y su variabilidad")
plt.xlabel("Coeficiente de importancia")
plt.show()

"""**Empaquetar el algoritmo**

Identificamos en el gráfico anterior, que el ser fumador o no es la variable más relevante y por ende el más importante.
"""

import pickle

with open('model.pkl', 'wb') as file:
  pickle.dump(model, file)

with open('model.pkl', 'rb') as file:
  model = pickle.load(file)

"""**Disponibilizar el algoritmo como API utilizando Flask dentro de un Contenedor y subirlo a
Docker**
"""

from flask import Flask, request, jsonify

#Inciciamos la aplicación Flask y definimos la ruta y el método para hacer la predicción
app = Flask(__name__)
@app.route('/predict', methods=['POST'])

def predict():
    data = request.json
    prediction = model.predict([data['features']])
    response = {'prediction': prediction.tolist()}
    return jsonify(response)

if __name__ == '_main_':
  app.run()

"""**Crear un entorno tox de pruebas dentro del repositorio que se empaquetó**"""

!pip install tox

!pip install pytest

!tox

